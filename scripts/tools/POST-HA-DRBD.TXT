Nodo 1: Principal: 172.16.10.200 (tr치fico PGSQL); Secundaria (tr치fico DRBD): 172.16.11.89
Nodo 2: Principal: 172.16.10.101 (tr치fico PGSQL); Secundaria (tr치fico DRBD): 172.16.11.90


resource postgresql {
    protocol C;
    disk {
        # on I/O errors, detach device
        on-io-error detach;
    }

    meta-disk internal;
    device /dev/drbd1;

    syncer {
        verify-alg sha1;
        rate 40M;
    }

    net {
        allow-two-primaries;
    }

    on FQDN-DNS-NODE1.dom {
        device /dev/drbd0;
        disk /dev/vdb;
        address IP-NODE1-ETH1:7789;
        meta-disk internal;
    }
    on FQDN-DNS-NODE2.dom {
        device /dev/drbd0;
        disk /dev/vdb;
        address IP-NODE1-ETH2:7789;
        meta-disk internal;
    }
}

NOTA: El apartado disk /dev/vdb aplica unicamente para particiones efimeras creadas desde el OpenStack, en el caso de un Fisical Server se debe tener en cuenta que se debe crear la particion respectiva como /dev/sdax, adicionalmente se debe tener en cuenta que el ratio esta en 40M (rate 40M; )debido a que cuando el trafico viaja a traves de un enlace compartido se debe tomar entre un aprox del 30% para prevenir desastres, sin embargo en el caso que exista un enlace dedicado a traves de cable CrossOver se puede usar hasta el 95% del ancho de banda, siendo una conexion de 1 Gbps podrias tomar 110M Ejemplo Caso P2P con servidor fisico:

resource postgresql {
    protocol C;
    disk {
        # on I/O errors, detach device
        on-io-error detach;
    }

    meta-disk internal;
    device /dev/drbd1;

    syncer {
        verify-alg sha1;
        rate 110M;
    }

    net {
        allow-two-primaries;
    }

    on dbcluster01.hc.p2p.dom {
        device /dev/drbd0;
        disk /dev/sda3;
        address 10.1.150.201:7789;
        meta-disk internal;
    }
    on dbcluster02.hc.p2p.dom {
        device /dev/drbd0;
        disk /dev/sda3;
        address 10.1.150.202:7789;
        meta-disk internal;
    }


}



NODO 1 -HA
logfile /var/log/ha-log
logfacility local0
keepalive 2
deadtime 30
initdead 120
ucast eth2 10.1.151.202
udpport 694
auto_failback on
node dbcluster01.hc.p2p.dom
node dbcluster02.hc.p2p.dom

NODO 2-HA

debug 1
debugfile /var/log/ha-debug
logfile /var/log/ha-log
logfacility local0
keepalive 2
deadtime 30
initdead 120
ucast eth2 10.1.151.201
udpport 694
auto_failback on
node dbcluster01.hc.p2p.dom
node dbcluster02.hc.p2p.dom

